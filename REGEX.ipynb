{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master = \"yarn\"\n",
    "launcher.conf.spark.app.name = \"regex \"\n",
    "launcher.conf.spark.yarn.queue=\"root.analyst.editor-bu\"\n",
    "launcher.conf.spark.executor.cores=5\n",
    "launcher.conf.spark.executor.memory=\"15g\"\n",
    "launcher.conf.spark.driver.memory=\"10g\"\n",
    "launcher.conf.spark.dynamicAllocation.enabled=\"true\"\n",
    "launcher.conf.spark.shuffle.service.enabled=\"true\"\n",
    "launcher.conf.spark.dynamicAllocation.maxExecutors=50\n",
    "launcher.jars=[\"/opt/shared/postgresql_1.jar\"]\n",
    "launcher.conf.spark.sql.shuffle.partitions=250\n",
    "launcher.conf.spark.serializer=\"org.apache.spark.serializer.KryoSerializer\"\n",
    "launcher.conf.spark.yarn.executor.memoryOverhead=\"5120m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.joda.time.format.DateTimeFormat\n",
       "import java.util.Properties\n",
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.{DataFrame, SQLContext}\n",
       "import org.apache.spark.{SparkConf, SparkContext}\n",
       "import org.joda.time.{DateTime, Days}\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "import org.apache.spark.sql.SaveMode._\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.SparkSession\n",
       "import org.json4s._\n",
       "import org.json4s.jackson.JsonMethods._\n",
       "import org.joda.time.format.DateTimeFormat\n",
       "import org.joda.time.{DateTime, Days}\n",
       "formatter: org.joda.time.format.DateTimeFormatter = org.joda.time.format.DateTimeFormatter@66263b58\n",
       "import scala.util.Try\n",
       "import java.sql.{Connection, DriverManager, ResultSet}\n",
       "sparkSession: o..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.joda.time.format.DateTimeFormat\n",
    "import java.util.Properties\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.{DataFrame, SQLContext}\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.joda.time.{DateTime, Days}\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.SaveMode._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.json4s._\n",
    "import org.json4s.jackson.JsonMethods._\n",
    "import org.joda.time.format.DateTimeFormat\n",
    "import org.joda.time.{DateTime, Days}\n",
    "val formatter = DateTimeFormat.forPattern(\"yyyy-MM-dd\")\n",
    "\n",
    "import scala.util.Try\n",
    "import java.sql.{Connection, DriverManager, ResultSet}\n",
    "val sparkSession = SparkSession.builder.master(\"local\").appName(\"example\").getOrCreate()\n",
    "import sparkSession.implicits._      \n",
    "import org.apache.spark.SparkContext  \n",
    "\n",
    "\n",
    "val spark = SparkSession.builder().appName(\"test\").getOrCreate()\n",
    "\n",
    "val pathFormatter = DateTimeFormat.forPattern(\"yyyy/MM/dd\")\n",
    "val partitionFormatter = DateTimeFormat.forPattern(\"yyyy-MM-dd\")\n",
    "\n",
    "val props = new Properties()\n",
    "val JDBC_URL = \"jdbc:postgresql://172.16.33.44:5432/dwh\"\n",
    "\n",
    "props.setProperty(\"driver\", \"org.postgresql.Driver\")\n",
    "props.setProperty(\"max_connections\", \"10000\")\n",
    "props.setProperty(\"user\", \"dwh\")\n",
    "props.setProperty(\"password\", \"4F51hnXVMZoDcHrLvf\")\n",
    "props.setProperty(\"loginTimeout\", \"30\")\n",
    "props.setProperty(\"socketTimeout\", \"1800\")\n",
    "\n",
    "\n",
    "val sqlContext = spark\n",
    "\n",
    "\n",
    "def publish(df:DataFrame, table:String, append:Boolean): Unit = {\n",
    "    val conn = DriverManager.getConnection(\"jdbc:postgresql://172.16.33.44:5432/dwh?user=dwh&password=4F51hnXVMZoDcHrLvf\")\n",
    "    try {\n",
    "        val mode = if(append) Append else Overwrite\n",
    "        df.write.mode(mode).jdbc(JDBC_URL, table, props)\n",
    "    }\n",
    "    catch {\n",
    "        case e: Exception => \n",
    "        e.printStackTrace()\n",
    "    }\n",
    "    finally {\n",
    "        conn.close\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def getBigTable(from:DateTime, to:DateTime) : DataFrame = {\n",
    "  val days = Days.daysBetween(from.withTimeAtStartOfDay(), to.withTimeAtStartOfDay()).getDays\n",
    "  val files = (0 to days).map { d=>\n",
    "    val dateStr = from.plusDays(d).toString(partitionFormatter)\n",
    "    s\"/analytics/big-table/partition_date=$dateStr\"\n",
    "  }\n",
    "  spark.read.parquet(files:_*)\n",
    "}\n",
    "\n",
    "def getEvent(app:String, event:String, from:DateTime, to:DateTime, mergeSchemaOption: String = \"false\") : DataFrame = {\n",
    "  val days = Days.daysBetween(from.withTimeAtStartOfDay(), to.withTimeAtStartOfDay()).getDays\n",
    "  val files = (0 to days).map { d=>\n",
    "    val dateStr = from.plusDays(d).toString(pathFormatter)\n",
    "    s\"/analytics/events/PARQUET/mobile_events/$app/$dateStr/$event\"\n",
    "  }\n",
    "  spark.read.option(\"mergeSchema\", mergeSchemaOption).parquet(files:_*)\n",
    "}\n",
    "\n",
    "def getSocialEvent(from:DateTime, to:DateTime) : DataFrame = {\n",
    "  val days = Days.daysBetween(from.withTimeAtStartOfDay(), to.withTimeAtStartOfDay()).getDays\n",
    "  val files = (0 to days).map { d=>\n",
    "    val dateStr = from.plusDays(d).toString(pathFormatter)\n",
    "    s\"/analytics/events/PARQUET/social_events/$dateStr/\"\n",
    "  }\n",
    "  spark.read.parquet(files:_*)\n",
    "}\n",
    "\n",
    "def getActiveDevices(from:DateTime, to:DateTime): DataFrame = {\n",
    "  val days = Days.daysBetween(from.withTimeAtStartOfDay(), to.withTimeAtStartOfDay()).getDays\n",
    "  val files = (0 to days).map { d=>\n",
    "    val dateStr = from.plusDays(d).toString(pathFormatter)\n",
    "    s\"/analytics/events/PARQUET/mobile_devices/$dateStr/\"\n",
    "  }\n",
    "  spark.read.parquet(files:_*)\n",
    "}\n",
    "\n",
    "def table(app:String, event:String, from:DateTime, to:DateTime) : Unit = {\n",
    "  val days = Days.daysBetween(from.withTimeAtStartOfDay(), to.withTimeAtStartOfDay()).getDays\n",
    "  val files = (0 to days).map { d=>\n",
    "    val dateStr = from.plusDays(d).toString(pathFormatter)\n",
    "    s\"/analytics/events/PARQUET/mobile_events/$app/$dateStr/$event\"\n",
    "  }\n",
    "  spark.read.parquet(files:_*).registerTempTable(event)\n",
    "}\n",
    "\n",
    "def getEntity(entity: String): DataFrame = {\n",
    "  spark.read.parquet(s\"/analytics/entities/$entity\")\n",
    "}\n",
    "\n",
    "def getUsers() : DataFrame = {\n",
    "  getEntity(\"users\")\n",
    "}\n",
    "\n",
    "def getPhotos() : DataFrame = {\n",
    "  getEntity(\"photos\")\n",
    "}\n",
    "\n",
    "def getContests() : DataFrame = {\n",
    "  getEntity(\"contests\")\n",
    "}\n",
    "\n",
    "def getTags() : DataFrame = {\n",
    "  getEntity(\"tags\")\n",
    "}\n",
    "\n",
    "def getStreams() : DataFrame = {\n",
    "  getEntity(\"streams\")\n",
    "}\n",
    "\n",
    "def getDevices() : DataFrame = {\n",
    "  getEntity(\"device_attributes\")\n",
    "}\n",
    "\n",
    "def getRequests() : DataFrame = {\n",
    "  getEntity(\"requests\")\n",
    "}\n",
    "\n",
    "def getCommon(app:String, from:DateTime, to:DateTime) : DataFrame = {\n",
    "  getEvent(app, \"common\", from, to)\n",
    "}\n",
    "\n",
    "\n",
    "def today(): DateTime = {\n",
    "  new DateTime().withTimeAtStartOfDay()\n",
    "}\n",
    "\n",
    "def yesterday(): DateTime = {\n",
    "  today().minusDays(1)\n",
    "}\n",
    "\n",
    "def getCommonLastNDays(app:String, days:Int) : org.apache.spark.sql.DataFrame = {\n",
    "  getEvent(app, \"common\", today().minusDays(days), today())\n",
    "}\n",
    "\n",
    "def getEventLastNDays(app:String, event:String, days:Int) : DataFrame = {\n",
    "  getEvent(app, event, today().minusDays(days), today())\n",
    "}\n",
    "\n",
    "def getStringParam(name:String, default:String): String = {\n",
    "  Try(System.getenv(name)).getOrElse(default)\n",
    "}\n",
    "\n",
    "def getLongParam(name:String, default:Long): Long = {\n",
    "  Try(System.getenv(name).toLong).getOrElse(default)\n",
    "}\n",
    "\n",
    "def getDateParam(name:String, default:DateTime): DateTime = {\n",
    "  Try(pathFormatter.parseDateTime(System.getenv(name))).getOrElse(default)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def getMobileDevices(from:DateTime, to:DateTime): DataFrame = {\n",
    "  val days = Days.daysBetween(from.withTimeAtStartOfDay(), to.withTimeAtStartOfDay()).getDays\n",
    "  val files = (0 to days).map { d=>\n",
    "    val dateStr = from.plusDays(d).toString(pathFormatter)\n",
    "    s\"/analytics/events/PARQUET/mobile_devices/$dateStr/\"\n",
    "  }\n",
    "  spark.read.parquet(files:_*)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def union_events(event:Array[(String,String)], from:DateTime, to:DateTime) : DataFrame = {\n",
    "  \n",
    "  var union_base : org.apache.spark.sql.DataFrame = null\n",
    "  var query:String=\"\"\n",
    "  \n",
    "   for(d<-event)\n",
    "{\n",
    "    \n",
    "     if (d._2!=\"\")\n",
    "     {query=\"where \"+d._2}\n",
    "    else\n",
    "    query=\"\"\n",
    "\n",
    "    var second=getEvent(\"com.picsart.studio\",d._1,from,to).registerTempTable(\"second\")\n",
    "\n",
    "    var final_second = spark.sql(s\"\"\" select * from second  $query \"\"\").\n",
    "    select($\"device_id\",$\"platform\",to_date($\"timestamp\").as(\"date\"),lower($\"country_code\").as(\"country_code\"))  \n",
    "\n",
    "if(union_base==null) \n",
    "    {union_base=final_second} \n",
    "else \n",
    "    {union_base=union_base.unionAll(final_second)}\n",
    "\n",
    "}\n",
    "return union_base\n",
    "}\n",
    " \n",
    " \n",
    "def erase_table(table_name:String, condition:String =\"\") {\n",
    "   var JDBC_DRIVER = \"org.postgresql.Driver\";  \n",
    "   var DB_URL = \"jdbc:postgresql://172.16.33.44:5432/dwh\";\n",
    "   var USER = \"dwh\";\n",
    "   var PASS = \"4F51hnXVMZoDcHrLvf\";\n",
    "   var conn:java.sql.Connection = null;\n",
    "   var stmt:java.sql.Statement = null;\n",
    "   conn = java.sql.DriverManager.getConnection(DB_URL, USER, PASS);\n",
    "   stmt = conn.createStatement();\n",
    "   var sql:String = s\"DELETE FROM $table_name\" ;\n",
    "   if (condition != \"\") { sql = sql+ \" where \" + condition}\n",
    "   println(sql)\n",
    "   stmt.executeUpdate(sql);\n",
    "   stmt.close()\n",
    "}\n",
    "\n",
    "def getActive(from:DateTime, to:DateTime): DataFrame = {\n",
    "\n",
    " val days = Days.daysBetween(from.withTimeAtStartOfDay(), to.withTimeAtStartOfDay()).getDays\n",
    " var union_base : org.apache.spark.sql.DataFrame = null\n",
    "(0 to days).map { d=>\n",
    "val dateStr = from.plusDays(d).toString(pathFormatter) \n",
    "var aa=spark.read.parquet(s\"/analytics/events/PARQUET/mobile_devices/$dateStr/\").\n",
    "filter($\"app\"===\"com.picsart.studio\").\n",
    "select(\"device_id\").distinct.\n",
    "withColumn(\"date\",lit(dateStr)).\n",
    "withColumn(\"date\", regexp_replace(col(\"date\"), \"/\", \"-\")).\n",
    "groupBy(to_date($\"date\").as(\"date\")).agg(countDistinct(\"device_id\"))\n",
    "\n",
    "   \n",
    "\n",
    "if(union_base==null) \n",
    "    {union_base=aa} \n",
    "else \n",
    "    {union_base=union_base.unionAll(aa)}\n",
    "\n",
    "}\n",
    "return union_base\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|text_content|\n",
      "+------------+\n",
      "|❤️          |\n",
      "+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import java.util.regex.Pattern\n",
       "to: org.joda.time.DateTime = 2019-10-12T00:00:00.000Z\n",
       "from: org.joda.time.DateTime = 2019-09-12T00:00:00.000Z\n",
       "text: Unit = ()\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.regex.Pattern\n",
    "\n",
    "// val regex = \"[^\\\\p{L}\\\\p{N}\\\\p{P}\\\\p{Z}\\\\n\\\\t\\\\r]\"\n",
    "// val regex = \"(\\\\Q:)\\\\E|\\\\Q:D\\\\E|\\\\Q:(\\\\E|\\\\Q:wink:\\\\E)\"\n",
    "// val pattern = Pattern.compile(regex, Pattern.UNICODE_CASE)\n",
    "// val matcher = pattern.matcher(\" \")\n",
    "\n",
    "\n",
    "val to = DateTime.parse(\"2019-10-12\")\n",
    "val from = DateTime.parse(\"2019-09-12\")\n",
    "\n",
    "val text = getEvent(\"com.picsart.studio\", \"edit_text_apply\", from, to).\n",
    "filter($\"text_content\".isin(\"❤️\") && $\"platform\" === \"android\").\n",
    "select($\"text_content\").distinct().\n",
    "show(100,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
